# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD 3-Clause license found in the
# LICENSE file in the root directory of this source tree.

import datetime
import importlib.util
import logging
import os
import sys
import traceback
from typing import Callable, Dict, List
from BackendBench.eval import _allclose

import torch

from .base import Backend
from BackendBench.llm_client import LLMKernelGenerator
from BackendBench.utils import extract_operator_name

logger = logging.getLogger(__name__)


class LLMBackend(Backend):
    """
    Backend that uses LLMKernelGenerator to communicate with local plugboard server or direct Anthropic API.
    """

    def __init__(self, model: str, llm_client: LLMKernelGenerator) -> None:
        super().__init__("llm")
        self.compiled_kernels: Dict[str, Callable] = {}
        self.model = model
        self.llm_client = llm_client
        # Create generated_kernels directory
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.kernels_dir = f"generated_kernels/llm_run_{timestamp}"
        os.makedirs(self.kernels_dir, exist_ok=True)
        server_description = llm_client.readme_server_description
        setup_section = llm_client.readme_setup_section

        # Create README for this run
        readme_path = os.path.join(self.kernels_dir, "README.md")
        with open(readme_path, "w") as f:
            f.write(
                f"""# Generated Kernels - LLM - {timestamp}

This directory contains PyTorch/Triton kernels generated by the LLM Backend.

## Run Info
- Timestamp: {timestamp}
- Backend: LLM
- Model: {model}
- Server: {server_description}

## Files
Each `<op_name>_kernel.py` file contains the complete generated kernel code for that operation, including:
- All necessary imports
- Triton kernel implementation (if applicable)
- Wrapper function that matches PyTorch operation signature

{setup_section}

## Usage
You can inspect these files to debug kernel generation, manually test implementations, or understand what the LLM produced.
"""
            )

        logger.info(f"Saving LLM generated kernels to: {self.kernels_dir}")

    def compile_kernel_from_string(
        self, kernel_code: str, op_name: str, attempt: int = 1
    ) -> Callable:
        """Compile a kernel from string code and return a callable."""
        try:
            is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code

            if is_triton:
                full_code = self._prepare_triton_code(kernel_code)
            else:
                full_code = self._prepare_torch_code(kernel_code)

            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")
            with open(kernel_file, "w") as f:
                f.write(full_code)

            logger.debug(f"Saved kernel to: {kernel_file}")

            spec = importlib.util.spec_from_file_location(f"kernel_{op_name}", kernel_file)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            kernel_func = self._find_kernel_function(module, op_name)

            return kernel_func

        except Exception as e:
            raise RuntimeError(f"Failed to compile kernel for {op_name}: {str(e)}")

    def _prepare_triton_code(self, kernel_code: str) -> str:
        """Prepare Triton kernel code with necessary imports."""
        imports = """
import torch
import triton
import triton.language as tl
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _prepare_torch_code(self, kernel_code: str) -> str:
        """Prepare regular PyTorch kernel code with necessary imports."""
        imports = """
import torch
import torch.nn.functional as F
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _find_kernel_function(self, module, op_name: str) -> Callable:
        """Find the main kernel function in the compiled module."""
        expected_name = f"{op_name}_kernel_impl"

        if hasattr(module, expected_name):
            return getattr(module, expected_name)

        available_functions = [
            name
            for name in dir(module)
            if callable(getattr(module, name)) and not name.startswith("_")
        ]

        raise ValueError(
            f"Expected function '{expected_name}' not found in kernel code for {op_name}. "
            f"Available functions: {available_functions}. "
            f"Please ensure the LLM generated code follows the naming convention: {op_name}_kernel_impl"
        )

    def _make_error_func(error_msg):
        def error_func(*args, **kwargs):
            raise RuntimeError(f"Compilation of kernel failed: {error_msg}")

        return error_func

    def add_kernel(self, op, kernel_code: str, op_name: str):
        """Add a kernel implementation for a specific operator."""

        try:
            compiled_kernel = self.compile_kernel_from_string(kernel_code, op_name, attempt=1)
            self.compiled_kernels[op] = compiled_kernel
        except Exception as e:
            self.compiled_kernels[op] = self._make_error_func(str(e))

    def test_kernel_correctness(
        self, op, kernel_code: str, test_cases: List, attempt: int = 1
    ) -> tuple[bool, Dict]:
        """Test kernel correctness and return detailed feedback."""
        op_str = str(op)
        if "aten." in op_str:
            op_name = op_str.split("aten.")[-1].split(".")[0]
        else:
            op_name = op_str.split(".")[-1]

        feedback_info = {
            "compilation_error": None,
            "test_errors": [],
            "summary": None,
        }

        try:
            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")

            if not os.path.exists(kernel_file):
                is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code
                if is_triton:
                    full_code = self._prepare_triton_code(kernel_code)
                else:
                    full_code = self._prepare_torch_code(kernel_code)

                with open(kernel_file, "w") as f:
                    f.write(full_code)
                logger.debug(f"Saved kernel to: {kernel_file}")

            spec = importlib.util.spec_from_file_location(
                f"test_kernel_{op_name}_{attempt}", kernel_file
            )
            module = importlib.util.module_from_spec(spec)

            # Add to sys.modules so triton can find it
            sys.modules[f"test_kernel_{op_name}_{attempt}"] = module

            try:
                spec.loader.exec_module(module)

                expected_name = f"{op_name}_kernel_impl"
                if hasattr(module, expected_name):
                    compiled_kernel = getattr(module, expected_name)
                else:
                    available_functions = [
                        name
                        for name in dir(module)
                        if callable(getattr(module, name)) and not name.startswith("_")
                    ]
                    raise ValueError(
                        f"Expected function '{expected_name}' not found. Available: {available_functions}"
                    )

            finally:
                if f"test_kernel_{op_name}_{attempt}" in sys.modules:
                    del sys.modules[f"test_kernel_{op_name}_{attempt}"]

                # Clear CUDA cache and synchronize to prevent memory buildup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

            correct_count = 0
            total_count = 0

            for test in test_cases:
                total_count += 1
                try:
                    args = test.args
                    kwargs = test.kwargs

                    ref_result = op(*args, **kwargs)

                    # Clear CUDA cache after running each kernel to prevent grabbing previous solutions
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    kernel_result = compiled_kernel(*args, **kwargs)

                    _allclose(ref_result, kernel_result)
                    correct_count += 1
                    logger.debug(f"    ✓ Test passed: {ref_result.shape} {ref_result.dtype}")

                except Exception as e:
                    logger.debug(f"    ✗ Test failed: {str(e)}")

                    feedback_info["test_errors"].append(
                        {
                            "test_input": f"args={[arg.shape if hasattr(arg, 'shape') else arg for arg in args]}, kwargs={kwargs}",
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "traceback": traceback.format_exc(),
                        }
                    )

                finally:
                    # Clean up memory by deleting args and kwargs if they exist
                    if "args" in locals():
                        del args
                    if "kwargs" in locals():
                        del kwargs

            is_correct = correct_count == total_count and total_count > 0
            feedback_info["summary"] = f"{correct_count}/{total_count} tests passed"

            return is_correct, feedback_info

        except Exception as e:
            logger.error("    ✗ Compilation failed:")
            logger.error(f"      Error: {str(e)}")

            feedback_info["compilation_error"] = str(e)
            feedback_info["summary"] = "Compilation failed"
            return False, feedback_info

    def __getitem__(self, key):
        if key in self.compiled_kernels:
            return self.compiled_kernels[key]
        raise KeyError(f"No kernel implementation found for {key}")

    def __contains__(self, key):
        return key in self.compiled_kernels

    def _write_summary(
        self, summary_file, op_name, op_str, attempts_used, max_attempts, llm_client
    ):
        with open(summary_file, "w") as f:
            f.write(f"Operation: {op_name}\n")
            f.write(f"Full op: {op_str}\n")
            f.write(f"Attempts used: {attempts_used}/{max_attempts}\n")
            f.write(f"Model: {llm_client.model}\n")
            f.write(f"Server: {llm_client.readme_server_description}\n")
            f.write(f"Final kernel file: {op_name}_kernel_attempt_{attempts_used}.py\n")

    def generate_kernels(self, suite, max_attempts=5):
        # Generate kernels for all operators in the suite
        successful_ops = 0
        total_ops = 0
        for op_test in suite:
            op = op_test.op
            total_ops += 1
            op_str = str(op)
            op_name = extract_operator_name(op_str)

            op_signature = f"def {op_name}(*args, **kwargs) -> torch.Tensor"
            op_description = f"PyTorch operation: {op_name}"

            logging.info(
                f"\n[{total_ops}] Generating kernel for {op_name} (full op: {op_str}) with up to {max_attempts} attempts"
            )

            # Create feedback callback
            def feedback_callback(kernel_code: str, attempt: int) -> tuple[bool, Dict]:
                # TODO: Add performance testing in addition to correctness testing
                return self.test_kernel_correctness(
                    op, kernel_code, op_test.correctness_tests, attempt
                )

            # Generate kernel with iterative refinement
            kernel_code, attempts_used, success = self.llm_client.generate_kernel_with_retry(
                op_name,
                op_signature,
                op_description,
                framework="triton",
                max_attempts=max_attempts,
                feedback_callback=feedback_callback,
            )
            self.add_kernel(op, kernel_code, op_name)
            if success:
                logging.info(
                    f"✓ Successfully generated and compiled kernel for {op_name} after {attempts_used} attempts"
                )
                successful_ops += 1
            else:
                logging.info(
                    f"✗ Failed to generate and compile kernel for {op_name} after {attempts_used} attempts"
                )

            summary_file = os.path.join(self.kernels_dir, f"{op_name}_summary.txt")
            self._write_summary(
                summary_file, op_name, op_str, attempts_used, max_attempts, self.llm_client
            )

        failed_ops = total_ops - successful_ops
        success_rate = f"{successful_ops / total_ops * 100:.1f}%" if total_ops > 0 else "0.0%"
        separator_line = "=" * 60

        # Console output format
        output_lines = [
            f"\n{separator_line}",
            "LLM BACKEND SETUP SUMMARY",
            separator_line,
            f"Total operations attempted: {total_ops}",
            f"Successfully created correct kernels for: {successful_ops} ops",
            f"Failed to create correct kernels for: {failed_ops} ops",
            f"Success rate: {success_rate}",
            f"Model used: {self.llm_client.model}",
            f"Server: {self.llm_client.readme_server_description}",
            f"Generated kernels saved to: {self.kernels_dir}",
            f"Backend: LLM \n{separator_line}\n",
        ]

        # Print summary
        for line in output_lines:
            logging.info(line)

        # Save overall summary
        overall_summary_file = os.path.join(self.kernels_dir, "OVERALL_SUMMARY.txt")
        with open(overall_summary_file, "w") as f:
            f.write("\n".join(output_lines))
