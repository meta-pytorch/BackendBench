# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD 3-Clause license found in the
# LICENSE file in the root directory of this source tree.

import datetime
import importlib.util
import logging
import os
import sys
import traceback
from typing import Callable, Dict, List

import torch

from BackendBench.llm_client import LLMKernelGenerator
from BackendBench.multiprocessing_eval import MultiprocessingEvaluator
from BackendBench.utils import extract_operator_name, compile_kernel_from_string

from .base import Backend

logger = logging.getLogger(__name__)


class PickleableKernel:
    def __init__(self, kernel_file, op_name, attempt):
        self.kernel_file = kernel_file
        self.op_name = op_name
        self.attempt = attempt
        self._load_kernel()

    def _load_kernel(self):
        import importlib.util
        import sys

        module_name = f"{self.op_name}_implementation_v{self.attempt}"
        spec = importlib.util.spec_from_file_location(module_name, self.kernel_file)
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        spec.loader.exec_module(module)

        expected_name = f"{self.op_name}_kernel_impl"
        self.kernel = getattr(module, expected_name)
        self._module = module  # Keep reference

    def __call__(self, *args, **kwargs):
        return self.kernel(*args, **kwargs)

    def __getstate__(self):
        # Return only the serializable parts
        return {
            "kernel_file": self.kernel_file,
            "op_name": self.op_name,
            "attempt": self.attempt,
        }

    def __setstate__(self, state):
        # Reconstruct the kernel in the new process
        self.kernel_file = state["kernel_file"]
        self.op_name = state["op_name"]
        self.attempt = state["attempt"]
        self._load_kernel()


class LLMBackend(Backend):
    """
    Backend that uses LLMKernelGenerator to communicate with local plugboard server or direct Anthropic API.
    """

    def __init__(self, model: str, llm_client: LLMKernelGenerator) -> None:
        super().__init__("llm")
        self.compiled_kernels: Dict[str, Callable] = {}
        self.model = model
        self.llm_client = llm_client
        # Create generated_kernels directory
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.kernels_dir = f"generated_kernels/llm_run_{timestamp}"
        os.makedirs(self.kernels_dir, exist_ok=True)
        server_description = llm_client.readme_server_description
        setup_section = llm_client.readme_setup_section

        # Create README for this run
        readme_path = os.path.join(self.kernels_dir, "README.md")
        with open(readme_path, "w") as f:
            f.write(
                f"""# Generated Kernels - LLM - {timestamp}

This directory contains PyTorch/Triton kernels generated by the LLM Backend.

## Run Info
- Timestamp: {timestamp}
- Backend: LLM
- Model: {model}
- Server: {server_description}

## Files
Each `<op_name>_kernel.py` file contains the complete generated kernel code for that operation, including:
- All necessary imports
- Triton kernel implementation (if applicable)
- Wrapper function that matches PyTorch operation signature

{setup_section}

## Usage
You can inspect these files to debug kernel generation, manually test implementations, or understand what the LLM produced.
"""
            )

        logger.info(f"Saving LLM generated kernels to: {self.kernels_dir}")

    def compile_kernel_from_string(
        self, kernel_code: str, op_name: str, attempt: int = 1
    ) -> Callable:
        """Compile a kernel from string code and return a callable."""
        op_dir = os.path.join(self.kernels_dir, op_name)
        os.makedirs(op_dir, exist_ok=True)
        kernel_file_path = os.path.join(op_dir, f"{op_name}_implementation_v{attempt}.py")
        expected_fn_name = f"{op_name}_implementation_v{attempt}"

        try:
            kernel = compile_kernel_from_string(
                kernel_code=kernel_code,
                op_name=op_name,
                kernel_file_path=kernel_file_path,
                expected_fn_name=expected_fn_name,
            )
        except Exception as e:
            raise e
        return kernel

    def _make_error_func(error_msg):
        def error_func(*args, **kwargs):
            raise RuntimeError(f"Compilation of kernel failed: {error_msg}")

        return error_func

    def add_kernel(self, op, kernel_code: str, op_name: str):
        """Add a kernel implementation for a specific operator."""

        try:
            compiled_kernel = self.compile_kernel_from_string(kernel_code, op_name, attempt=1)
            self.compiled_kernels[op] = compiled_kernel
        except Exception as e:
            self.compiled_kernels[op] = self._make_error_func(str(e))

    def test_kernel_correctness(
        self, op, kernel_code: str, test_cases: List, attempt: int = 1
    ) -> tuple[bool, Dict]:
        """Test kernel correctness and return detailed feedback."""
        op_str = str(op)
        if "aten." in op_str:
            op_name = op_str.split("aten.")[-1].split(".")[0]
        else:
            op_name = op_str.split(".")[-1]

        feedback_info = {
            "compilation_error": None,
            "test_errors": [],
            "summary": None,
        }

        try:
            op_dir = os.path.join(self.kernels_dir, op_name)
            os.makedirs(op_dir, exist_ok=True)
            kernel_file = os.path.join(op_dir, f"{op_name}_implementation_v{attempt}.py")

            if not os.path.exists(kernel_file):
                is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code
                if is_triton:
                    full_code = self._prepare_triton_code(kernel_code)
                else:
                    full_code = self._prepare_torch_code(kernel_code)

                with open(kernel_file, "w") as f:
                    f.write(full_code)
                logger.debug(f"Saved kernel to: {kernel_file}")

            spec = importlib.util.spec_from_file_location(
                f"{op_name}_implementation_v{attempt}", kernel_file
            )
            module = importlib.util.module_from_spec(spec)

            # Add to sys.modules so triton can find it
            sys.modules[f"{op_name}_implementation_v{attempt}"] = module

            try:
                spec.loader.exec_module(module)

                expected_name = f"{op_name}_kernel_impl"
                if hasattr(module, expected_name):
                    # check if the kernel compile / is loadable
                    _ = getattr(module, expected_name)
                else:
                    available_functions = [
                        name
                        for name in dir(module)
                        if callable(getattr(module, name)) and not name.startswith("_")
                    ]
                    raise ValueError(
                        f"Expected function '{expected_name}' not found. Available: {available_functions}"
                    )

            finally:
                if f"test_kernel_{op_name}_{attempt}" in sys.modules:
                    del sys.modules[f"test_kernel_{op_name}_{attempt}"]

                # Clear CUDA cache and synchronize to prevent memory buildup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

            correct_count = 0
            total_count = 0
            correctness_results = []
            # todo: this is to protect against IMA errors, however, we should make this work / make sense with multiple workers
            with MultiprocessingEvaluator(1) as evaluator:
                loaded_kenrel = PickleableKernel(kernel_file, op_name, attempt)
                _ = evaluator.submit_task(
                    op,
                    loaded_kenrel,
                    test_cases,
                    [],
                )

                # Start evaluation
                evaluator.start_evaluation()
                # Get results
                results = evaluator.get_results()

            for result in results:
                correctness_results.extend(result.correctness_results)
            correct_results = [result for result in correctness_results if result.is_correct]
            total_count = len(correctness_results)
            failure_results = [result for result in correctness_results if not result.is_correct]
            correct_count = len(correct_results)

            for result in failure_results:
                feedback_info["test_errors"].append(
                    {
                        "test_input": result.args,
                        "error": result.error_msg,
                        "error_type": result.error_type,
                        "traceback": result.traceback,
                    }
                )

            is_correct = correct_count == total_count and total_count > 0
            feedback_info["summary"] = f"{correct_count}/{total_count} tests passed"

            return is_correct, feedback_info

        except Exception as e:
            logger.error("    ✗ Compilation failed:")
            logger.error(f"      Error: {str(e)}")
            logger.error("      Traceback:")
            logger.error(traceback.format_exc())

            feedback_info["compilation_error"] = str(e)
            feedback_info["summary"] = "Compilation failed"
            return False, feedback_info

    def __getitem__(self, key):
        if key in self.compiled_kernels:
            return self.compiled_kernels[key]
        raise KeyError(f"No kernel implementation found for {key}")

    def __contains__(self, key):
        return key in self.compiled_kernels

    def _write_summary(
        self,
        summary_file,
        op_name,
        op_str,
        attempts_used,
        max_attempts,
        llm_client,
        success,
    ):
        with open(summary_file, "w") as f:
            f.write(f"Operation: {op_name}\n")
            f.write(f"Full op: {op_str}\n")
            f.write(f"Attempts used: {attempts_used}/{max_attempts}\n")
            f.write(f"Final Status: {'✓ Success' if success else '✗ Failure'}\n")
            f.write(f"Model: {llm_client.model}\n")
            f.write(f"Server: {llm_client.readme_server_description}\n")
            f.write(f"Final kernel file: {op_name}_kernel_attempt_{attempts_used}.py\n")

    def generate_kernels(self, suite, max_attempts=5):
        # Generate kernels for all operators in the suite
        successful_ops = 0
        total_ops = 0
        for op_test in suite:
            op = op_test.op
            total_ops += 1
            op_str = str(op)
            op_name = extract_operator_name(op_str)

            op_signature = f"def {op_name}(*args, **kwargs) -> torch.Tensor"
            op_description = f"PyTorch operation: {op_name}"

            logging.info(
                f"\n[{total_ops}] Generating kernel for {op_name} (full op: {op_str}) with up to {max_attempts} attempts"
            )

            # Create feedback callback
            def feedback_callback(kernel_code: str, attempt: int) -> tuple[bool, Dict]:
                # TODO: Add performance testing in addition to correctness testing
                return self.test_kernel_correctness(
                    op, kernel_code, op_test.correctness_tests, attempt
                )

            # Generate kernel with iterative refinement
            kernel_code, attempts_used, success = self.llm_client.generate_kernel_with_retry(
                op_name,
                op_signature,
                op_description,
                framework="triton",
                max_attempts=max_attempts,
                feedback_callback=feedback_callback,
            )
            self.add_kernel(op, kernel_code, op_name)
            if success:
                logging.info(
                    f"✓ Successfully generated and compiled kernel for {op_name} after {attempts_used} attempts"
                )
                successful_ops += 1
            else:
                logging.info(
                    f"✗ Failed to generate and compile kernel for {op_name} after {attempts_used} attempts"
                )

            summary_file = os.path.join(self.kernels_dir, op_name, f"{op_name}_summary.txt")
            self._write_summary(
                summary_file,
                op_name,
                op_str,
                attempts_used,
                max_attempts,
                self.llm_client,
                success,
            )

        failed_ops = total_ops - successful_ops
        success_rate = f"{successful_ops / total_ops * 100:.1f}%" if total_ops > 0 else "0.0%"
        separator_line = "=" * 60

        # Console output format
        output_lines = [
            f"\n{separator_line}",
            "LLM BACKEND SETUP SUMMARY",
            separator_line,
            f"Total operations attempted: {total_ops}",
            f"Successfully created correct kernels for: {successful_ops} ops",
            f"Failed to create correct kernels for: {failed_ops} ops",
            f"Success rate: {success_rate}",
            f"Model used: {self.llm_client.model}",
            f"Server: {self.llm_client.readme_server_description}",
            f"Generated kernels saved to: {self.kernels_dir}",
            f"Backend: LLM \n{separator_line}\n",
        ]

        # Print summary
        for line in output_lines:
            logging.info(line)

        # Save overall summary
        overall_summary_file = os.path.join(self.kernels_dir, "OVERALL_SUMMARY.txt")
        with open(overall_summary_file, "w") as f:
            f.write("\n".join(output_lines))
