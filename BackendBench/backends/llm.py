# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD 3-Clause license found in the
# LICENSE file in the root directory of this source tree.

import importlib.util
import logging
import os
from typing import Callable, Dict, List, Optional

from .base import Backend
from ..conversation_manager import ConversationManager

logger = logging.getLogger(__name__)


class LLMBackend(Backend):
    def __init__(self, debug_mode: bool = False) -> None:
        super().__init__("llm")
        self.compiled_kernels: Dict[str, Callable] = {}
        self.debug_mode = debug_mode
        self.conversation_managers: Dict[str, ConversationManager] = {}

        # Create generated_kernels directory with conversation support
        import datetime

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.kernels_dir = f"generated_kernels/run_{timestamp}"
        os.makedirs(self.kernels_dir, exist_ok=True)

        # Create conversations subdirectory for debug logs
        if self.debug_mode:
            self.conversations_dir = os.path.join(self.kernels_dir, "conversations")
            os.makedirs(self.conversations_dir, exist_ok=True)

        # Create README for this run
        readme_path = os.path.join(self.kernels_dir, "README.md")
        with open(readme_path, "w") as f:
            f.write(
                f"""# Generated Kernels - {timestamp}

This directory contains PyTorch/Triton kernels generated by the LLM Backend.

## Run Info
- Timestamp: {timestamp}
- Backend: LLM
- Debug Mode: {debug_mode}
- Conversation History: {'Enabled' if debug_mode else 'Disabled'}

## Files
Each `<op_name>_kernel.py` file contains the complete generated kernel code for that operation, including:
- All necessary imports
- Triton kernel implementation (if applicable)
- Wrapper function that matches PyTorch operation signature

## Conversation Logs
{'Conversation logs saved to conversations/ subdirectory when debug mode is enabled.' if debug_mode else 'Enable debug mode to generate conversation logs.'}

## Usage
You can inspect these files to debug kernel generation, manually test implementations, or understand what the LLM produced.
"""
            )

        logger.info(f"Saving generated kernels to: {self.kernels_dir}")
        if debug_mode:
            logger.info(f"Debug mode enabled - conversation logs will be saved to: {self.conversations_dir}")

    def compile_kernel_from_string(
        self, kernel_code: str, op_name: str, attempt: int = 1
    ) -> Callable:
        """Compile a kernel from string code and return a callable."""
        try:
            is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code

            if is_triton:
                full_code = self._prepare_triton_code(kernel_code)
            else:
                full_code = self._prepare_torch_code(kernel_code)

            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")
            with open(kernel_file, "w") as f:
                f.write(full_code)

            print(f"Saved kernel to: {kernel_file}")

            spec = importlib.util.spec_from_file_location(f"kernel_{op_name}", kernel_file)
            if spec is None:
                raise RuntimeError(f"Failed to create module spec for {kernel_file}")

            module = importlib.util.module_from_spec(spec)
            if spec.loader is None:
                raise RuntimeError(f"No loader available for module spec {kernel_file}")

            spec.loader.exec_module(module)

            kernel_func = self._find_kernel_function(module, op_name)

            return kernel_func

        except Exception as e:
            raise RuntimeError(f"Failed to compile kernel for {op_name}: {str(e)}")

    def _prepare_triton_code(self, kernel_code: str) -> str:
        """Prepare Triton kernel code with necessary imports."""
        imports = """
import torch
import triton
import triton.language as tl
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _prepare_torch_code(self, kernel_code: str) -> str:
        """Prepare regular PyTorch kernel code with necessary imports."""
        imports = """
import torch
import torch.nn.functional as F
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _find_kernel_function(self, module, op_name: str) -> Callable:
        """Find the main kernel function in the compiled module."""
        expected_name = f"{op_name}_kernel_impl"

        if hasattr(module, expected_name):
            return getattr(module, expected_name)

        available_functions = [
            name
            for name in dir(module)
            if callable(getattr(module, name)) and not name.startswith("_")
        ]

        raise ValueError(
            f"Expected function '{expected_name}' not found in kernel code for {op_name}. "
            f"Available functions: {available_functions}. "
            f"Please ensure the LLM generated code follows the naming convention: {op_name}_kernel_impl"
        )

    def add_kernel(self, op, kernel_code: str, op_name: str):
        """Add a kernel implementation for a specific operator."""
        compiled_kernel = self.compile_kernel_from_string(kernel_code, op_name, attempt=1)
        self.compiled_kernels[op] = compiled_kernel

    def test_kernel_correctness(
        self, op, kernel_code: str, test_cases: List, attempt: int = 1
    ) -> tuple[bool, Dict]:
        """Test kernel correctness and return detailed feedback."""
        op_str = str(op)
        if "aten." in op_str:
            op_name = op_str.split("aten.")[-1].split(".")[0]
        else:
            op_name = op_str.split(".")[-1]

        # CRITICAL DEBUG: Check test_cases state
        try:
            test_cases_count = len(test_cases)
            print(f"ðŸ” test_kernel_correctness attempt {attempt}: received {test_cases_count} test cases")
        except:
            print(f"ðŸ” test_kernel_correctness attempt {attempt}: received test_cases (unknown count)")

        if not test_cases:
            print(f"âŒ CRITICAL BUG: test_cases is empty on attempt {attempt} for {op_name}")

        feedback_info = {
            "compilation_error": None,
            "test_errors": [],
            "summary": None,
        }

        try:
            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")

            # Always save the new kernel code for each attempt
            is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code
            if is_triton:
                full_code = self._prepare_triton_code(kernel_code)
            else:
                full_code = self._prepare_torch_code(kernel_code)

            with open(kernel_file, "w") as f:
                f.write(full_code)
            print(f"Saved kernel to: {kernel_file}")

            import importlib.util
            import sys

            spec = importlib.util.spec_from_file_location(
                f"test_kernel_{op_name}_{attempt}", kernel_file
            )
            if spec is None:
                raise RuntimeError(f"Failed to create module spec for {kernel_file}")

            module = importlib.util.module_from_spec(spec)
            if spec.loader is None:
                raise RuntimeError(f"No loader available for module spec {kernel_file}")

            # Add to sys.modules so triton can find it
            sys.modules[f"test_kernel_{op_name}_{attempt}"] = module

            try:
                spec.loader.exec_module(module)

                expected_name = f"{op_name}_kernel_impl"
                if hasattr(module, expected_name):
                    compiled_kernel = getattr(module, expected_name)
                else:
                    available_functions = [
                        name
                        for name in dir(module)
                        if callable(getattr(module, name)) and not name.startswith("_")
                    ]
                    raise ValueError(
                        f"Expected function '{expected_name}' not found. Available: {available_functions}"
                    )

            finally:
                if f"test_kernel_{op_name}_{attempt}" in sys.modules:
                    del sys.modules[f"test_kernel_{op_name}_{attempt}"]

            import torch

            correct_count = 0
            total_count = 0

            for test_idx, test in enumerate(test_cases):
                try:
                    # Create references to test data without mutating original
                    args = test.args
                    kwargs = test.kwargs

                    print(f"    Running test {test_idx + 1}/{len(test_cases)}")

                    ref_result = op(*args, **kwargs)
                    kernel_result = compiled_kernel(*args, **kwargs)

                    torch.testing.assert_close(ref_result, kernel_result, equal_nan=True)
                    correct_count += 1
                    print(f"    âœ“ Test {test_idx + 1} passed: {ref_result.shape} {ref_result.dtype}")

                except Exception as e:
                    import traceback

                    print(f"    âœ— Test {test_idx + 1} failed: {str(e)}")

                    # Safely get args/kwargs for error reporting
                    try:
                        test_input_info = f"args={[arg.shape if hasattr(arg, 'shape') else arg for arg in args]}, kwargs={kwargs}"
                    except:
                        test_input_info = f"Test case {test_idx + 1} (args/kwargs not available)"

                    feedback_info["test_errors"].append(
                        {
                            "test_input": test_input_info,
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "traceback": traceback.format_exc(),
                        }
                    )

                total_count += 1

            is_correct = correct_count == total_count and total_count > 0
            feedback_info["summary"] = f"{correct_count}/{total_count} tests passed"

            return is_correct, feedback_info

        except Exception as e:
            print("    âœ— Compilation failed:")
            print(f"      Error: {str(e)}")

            feedback_info["compilation_error"] = str(e)
            feedback_info["summary"] = "Compilation failed"
            return False, feedback_info

    def get_conversation_manager(self, op_name: str) -> Optional[ConversationManager]:
        """Get conversation manager for debugging."""
        return self.conversation_managers.get(op_name)

    def __getitem__(self, key):
        if key in self.compiled_kernels:
            return self.compiled_kernels[key]
        raise KeyError(f"No kernel implementation found for {key}")

    def __contains__(self, key):
        return key in self.compiled_kernels
