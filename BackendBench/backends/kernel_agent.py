# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD 3-Clause license found in the
# LICENSE file in the root directory of this source tree.

import logging
import os
from typing import Callable, Dict

from BackendBench.agent_errors import AgentError
from BackendBench.utils import compile_kernel_from_string

from .base import Backend

logger = logging.getLogger(__name__)


class KernelAgentBackend(Backend):
    """
    Backend that uses KernelAgent for sophisticated parallel kernel generation.

    This backend leverages KernelAgent's advanced features:
    - Parallel workers with iterative refinement
    - Multi-turn conversation history
    - Comprehensive prompt engineering with Triton guidelines
    - Automatic test generation
    """

    def __init__(self) -> None:
        super().__init__("kernel_agent")
        self.compiled_kernels: Dict[str, Callable] = {}

        # Create generated_kernels directory
        import datetime

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.kernels_dir = f"generated_kernels/kernel_agent_run_{timestamp}"
        os.makedirs(self.kernels_dir, exist_ok=True)

        # Create README for this run
        readme_path = os.path.join(self.kernels_dir, "README.md")
        with open(readme_path, "w") as f:
            f.write(
                f"""# Generated Kernels - KernelAgent - {timestamp}

This directory contains PyTorch/Triton kernels generated by the KernelAgent Backend.

## Run Info
- Timestamp: {timestamp}
- Backend: KernelAgent
- Features: Parallel workers, iterative refinement, conversation history

## Files
Each `<op_name>_kernel.py` file contains the complete generated kernel code for that operation.
KernelAgent session directories contain detailed logs, worker outputs, and generation artifacts.

## KernelAgent Features Used
- Parallel workers for increased success rate
- Iterative refinement with multi-turn dialogue
- Comprehensive Triton programming guidelines
- Automatic test generation and validation
- Session logging and artifact preservation

## Usage
You can inspect these files to debug kernel generation, analyze the parallel worker outputs,
or understand the sophisticated generation process used by KernelAgent.
"""
            )

        print(f"Saving KernelAgent generated kernels to: {self.kernels_dir}")

        self.kernel_agent = None
        self.num_workers = 4
        self.max_rounds = 10

    def set_config(self, num_workers: int, max_rounds: int):
        """Set configuration for KernelAgent."""
        self.num_workers = num_workers
        self.max_rounds = max_rounds

    def _get_kernel_agent(self):
        """Lazy initialization of KernelAgent to avoid import issues."""
        if self.kernel_agent is None:
            try:
                from triton_kernel_agent import TritonKernelAgent

                agent_log_dir = os.path.join(self.kernels_dir, "agent_logs")
                os.makedirs(agent_log_dir, exist_ok=True)

                self.kernel_agent = TritonKernelAgent(
                    log_dir=agent_log_dir,
                    num_workers=self.num_workers,
                    max_rounds=self.max_rounds,
                )

                print(f"‚úì KernelAgent initialized with log directory: {agent_log_dir}")

            except ImportError:
                raise ImportError(
                    "triton_kernel_agent package not found. Install it to use KernelAgent backend."
                )

        return self.kernel_agent

    def _create_problem_description_from_op(self, op, op_name: str) -> str:
        """
        Create a problem description for KernelAgent based on the PyTorch operation.

        Args:
            op: PyTorch operation
            op_name: Operation name extracted from op

        Returns:
            Problem description string for KernelAgent
        """
        # Create a comprehensive problem description that KernelAgent can understand
        problem_description = f"""
Implement a high-performance Triton kernel for the PyTorch operation: {op_name}

Operation details:
- PyTorch operation: {op}
- Operation name: {op_name}
- Framework target: OpenAI Triton

Requirements:
1. The kernel must be functionally equivalent to the PyTorch operation
2. Implement using Triton language primitives (tl.load, tl.store, etc.)
3. Handle all tensor shapes and data types that the original operation supports
4. Optimize for GPU performance with proper memory coalescing
5. Include proper boundary condition handling
6. Follow Triton best practices for kernel design

The generated kernel should:
- Take the same input arguments as the PyTorch operation
- Return outputs with identical shapes, dtypes, and numerical values
- Be optimized for common tensor shapes and memory layouts
- Handle edge cases gracefully

Please generate a complete, production-ready Triton kernel implementation.
"""
        return problem_description

    def _adapt_kernel_function_name(self, kernel_code: str, op_name: str) -> str:
        """
        Adapt KernelAgent's 'kernel_function' to BackendBench's expected naming convention.

        KernelAgent generates kernels with 'kernel_function' as the main entry point.
        BackendBench expects '{op_name}_kernel_impl' as the function name.

        Args:
            kernel_code: Original kernel code from KernelAgent
            op_name: Operation name for the expected function name

        Returns:
            Modified kernel code with correct function name
        """
        expected_name = f"{op_name}_kernel_impl"

        # Replace 'def kernel_function' with 'def {op_name}_kernel_impl'
        if "def kernel_function(" in kernel_code:
            adapted_code = kernel_code.replace("def kernel_function(", f"def {expected_name}(")

            # Also replace any docstring references
            adapted_code = adapted_code.replace(
                '"""Wrapper function that handles kernel launch."""',
                f'"""{op_name} kernel implementation using Triton."""',
            )

            return adapted_code
        else:
            # If kernel_function is not found, add a wrapper that calls the existing function
            wrapper_code = f'''

def {expected_name}(*args, **kwargs):
    """{op_name} kernel implementation using Triton - BackendBench adapter."""
    # Call the original kernel_function from KernelAgent
    return kernel_function(*args, **kwargs)
'''
            return kernel_code + wrapper_code

    def compile_kernel_from_string(
        self, kernel_code: str, op_name: str, attempt: int = 1
    ) -> Callable:
        """Compile a kernel from string code and return a callable."""
        adapted_code = self._adapt_kernel_function_name(kernel_code, op_name)
        kernel_file_path = os.path.join(self.kernels_dir, f"{op_name}_kernel.py")
        expected_fn_name = f"{op_name}_kernel_impl"
        module_name = f"kernel_agent_{op_name}"

        try:
            kernel = compile_kernel_from_string(
                kernel_code=adapted_code,
                op_name=op_name,
                kernel_file_path=kernel_file_path,
                expected_fn_name=expected_fn_name,
                module_name=module_name,
            )
        except Exception as e:
            raise e
        return kernel

    def add_kernel(self, op, kernel_code: str, op_name: str):
        """Add a kernel implementation for a specific operator."""
        compiled_kernel = self.compile_kernel_from_string(kernel_code, op_name, attempt=1)
        self.compiled_kernels[op] = compiled_kernel

        # Save the original KernelAgent code as well
        original_file = os.path.join(self.kernels_dir, f"{op_name}_original_kernel_agent.py")
        with open(original_file, "w") as f:
            f.write(kernel_code)

    def generate_kernel_with_agent(self, op, op_name: str) -> tuple[str, bool]:
        """
        Generate a kernel using KernelAgent's sophisticated generation system.

        Args:
            op: PyTorch operation
            op_name: Operation name

        Returns:
            tuple: (kernel_code, success)
        """
        try:
            agent = self._get_kernel_agent()
            problem_description = self._create_problem_description_from_op(op, op_name)
            print(
                f"üöÄ Generating {op_name} kernel with KernelAgent (parallel workers + refinement)"
            )

            result = agent.generate_kernel(
                problem_description=problem_description,
                test_code=None,
            )

            # Only raise AgentError if kernel_code is missing or malformed
            kernel_code = result.get("kernel_code")
            if not kernel_code or not isinstance(kernel_code, str):
                raise AgentError(f"Agent error: No kernel code produced for {op_name}.")

            if result["success"]:
                print(f"‚úÖ KernelAgent succeeded for {op_name}!")
                print(
                    f"   Worker {result['worker_id']} found solution in {result['rounds']} rounds"
                )
                print(f"   Session: {result['session_dir']}")
                import shutil

                session_name = os.path.basename(result["session_dir"])
                preserved_session = os.path.join(
                    self.kernels_dir, f"{op_name}_session_{session_name}"
                )
                try:
                    shutil.copytree(result["session_dir"], preserved_session)
                    print(f"   Session preserved: {preserved_session}")
                except Exception as e:
                    print(f"   Warning: Could not preserve session: {e}")
                return kernel_code, True
            else:
                # This is an agent output error, so raise AgentError
                raise AgentError(
                    f"Agent error: ‚ùå KernelAgent failed for {op_name}: {result['message']}"
                )

        except AgentError as e:
            print(f"‚ùå KernelAgent error for {op_name}: {e}")
            return "", False
        except Exception as e:
            # API/provider errors are not actionable by the agent
            print(f"‚ùå API/provider error for {op_name}: {e}")
            return "", False

    def __getitem__(self, key):
        if key in self.compiled_kernels:
            return self.compiled_kernels[key]
        raise KeyError(f"No KernelAgent kernel implementation found for {key}")

    def __contains__(self, key):
        return key in self.compiled_kernels
