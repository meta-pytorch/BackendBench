# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD 3-Clause license found in the
# LICENSE file in the root directory of this source tree.

import datetime
import gc
import importlib.util
import logging
import os
import sys
import torch
import traceback
from typing import Callable, Dict, List, Optional
import triton

from .base import Backend
from ..conversation_manager import ConversationManager

logger = logging.getLogger(__name__)


class LLMRelayBackend(Backend):
    """
    Backend that uses LLMKernelGenerator to communicate with local plugboard server.
    This backend will eventually replace the LLMBackend that uses direct Anthropic API calls.
    """

    def __init__(self, model: str = "gcp-claude-4-sonnet", debug_mode: bool = False) -> None:
        super().__init__("llm-relay")
        self.compiled_kernels: Dict[str, Callable] = {}
        self.model = model
        self.debug_mode = debug_mode
        self.conversation_managers: Dict[str, ConversationManager] = {}

        # Set CUDA environment variables for better error debugging
        os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
        os.environ["TORCH_USE_CUDA_DSA"] = "1"

        # Create generated_kernels directory with conversation support
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.kernels_dir = f"generated_kernels/llm_relay_run_{timestamp}"
        os.makedirs(self.kernels_dir, exist_ok=True)

        # Create conversations subdirectory for debug logs
        if self.debug_mode:
            self.conversations_dir = os.path.join(self.kernels_dir, "conversations")
            os.makedirs(self.conversations_dir, exist_ok=True)

        # Create README for this run
        readme_path = os.path.join(self.kernels_dir, "README.md")
        with open(readme_path, "w") as f:
            f.write(
                f"""# Generated Kernels - LLM Relay - {timestamp}

This directory contains PyTorch/Triton kernels generated by the LLM Relay Backend.

## Run Info
- Timestamp: {timestamp}
- Backend: LLM Relay
- Model: {model}
- Server: Local plugboard server (localhost:11434)
- Debug Mode: {debug_mode}
- Conversation History: {'Enabled' if debug_mode else 'Disabled'}

## Files
Each `<op_name>_kernel.py` file contains the complete generated kernel code for that operation, including:
- All necessary imports
- Triton kernel implementation (if applicable)
- Wrapper function that matches PyTorch operation signature

## Conversation Logs
{'Conversation logs saved to conversations/ subdirectory when debug mode is enabled.' if debug_mode else 'Enable debug mode to generate conversation logs.'}

## Server Setup
This backend requires the plugboard server to be running:
```
buck run @//mode/inplace run_plugboard_server -- --model gcp-claude-4-sonnet --pipeline usecase-dev-ai-user
```

## Usage
You can inspect these files to debug kernel generation, manually test implementations, or understand what the LLM produced.
"""
            )

        logger.info(f"Saving LLM Relay generated kernels to: {self.kernels_dir}")
        if debug_mode:
            logger.info(f"Debug mode enabled - conversation logs will be saved to: {self.conversations_dir}")

    def compile_kernel_from_string(
        self, kernel_code: str, op_name: str, attempt: int = 1
    ) -> Callable:
        """Compile a kernel from string code and return a callable."""
        try:
            is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code

            if is_triton:
                full_code = self._prepare_triton_code(kernel_code)
            else:
                full_code = self._prepare_torch_code(kernel_code)

            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")
            with open(kernel_file, "w") as f:
                f.write(full_code)

            logger.debug(f"Saved kernel to: {kernel_file}")

            spec = importlib.util.spec_from_file_location(f"kernel_{op_name}", kernel_file)
            if spec is None:
                raise RuntimeError(f"Failed to create module spec for {kernel_file}")

            module = importlib.util.module_from_spec(spec)
            if spec.loader is None:
                raise RuntimeError(f"No loader available for module spec {kernel_file}")

            spec.loader.exec_module(module)

            kernel_func = self._find_kernel_function(module, op_name)

            return kernel_func

        except Exception as e:
            raise RuntimeError(f"Failed to compile kernel for {op_name}: {str(e)}")

    def _prepare_triton_code(self, kernel_code: str) -> str:
        """Prepare Triton kernel code with necessary imports."""
        imports = """
import torch
import triton
import triton.language as tl
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _prepare_torch_code(self, kernel_code: str) -> str:
        """Prepare regular PyTorch kernel code with necessary imports."""
        imports = """
import torch
import torch.nn.functional as F
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _find_kernel_function(self, module, op_name: str) -> Callable:
        """Find the main kernel function in the compiled module."""
        expected_name = f"{op_name}_kernel_impl"

        if hasattr(module, expected_name):
            return getattr(module, expected_name)

        available_functions = [
            name
            for name in dir(module)
            if callable(getattr(module, name)) and not name.startswith("_")
        ]

        raise ValueError(
            f"Expected function '{expected_name}' not found in kernel code for {op_name}. "
            f"Available functions: {available_functions}. "
            f"Please ensure the LLM generated code follows the naming convention: {op_name}_kernel_impl"
        )

    def add_kernel(self, op, kernel_code: str, op_name: str):
        """Add a kernel implementation for a specific operator."""
        # Clear CUDA cache before final compilation to ensure clean state
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        compiled_kernel = self.compile_kernel_from_string(kernel_code, op_name, attempt=1)
        self.compiled_kernels[op] = compiled_kernel

        # Final cleanup after successful compilation
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def test_kernel_correctness(
        self, op, kernel_code: str, test_cases: List, attempt: int = 1
    ) -> tuple[bool, Dict]:
        """Test kernel correctness and return detailed feedback."""
        op_str = str(op)
        if "aten." in op_str:
            op_name = op_str.split("aten.")[-1].split(".")[0]
        else:
            op_name = op_str.split(".")[-1]

        # CRITICAL DEBUG: Check test_cases state
        try:
            test_cases_count = len(test_cases)
            print(f"ðŸ” test_kernel_correctness attempt {attempt}: received {test_cases_count} test cases")
        except:
            print(f"ðŸ” test_kernel_correctness attempt {attempt}: received test_cases (unknown count)")

        if not test_cases:
            print(f"âŒ CRITICAL BUG: test_cases is empty on attempt {attempt} for {op_name}")

        feedback_info = {
            "compilation_error": None,
            "test_errors": [],
            "summary": None,
        }

        # Clear CUDA cache at the beginning of each attempt to prevent memory buildup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            # Log current memory usage for debugging
            allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
            cached = torch.cuda.memory_reserved() / (1024**3)  # GB
            logger.debug(f"CUDA Memory before attempt {attempt}: Allocated={allocated:.2f}GB, Cached={cached:.2f}GB")

        # Clear Triton cache to prevent kernel accumulation - use nuclear approach
        try:
            # Clear Triton's internal kernel cache
            if hasattr(triton, 'cache'):
                triton.cache.clear()
            # Also clear any Triton compiler caches
            if hasattr(triton, 'compiler') and hasattr(triton.compiler, 'clear_cache'):
                triton.compiler.clear_cache()
            # Nuclear option: clear the entire triton compile cache directory
            try:
                import shutil
                triton_cache_dir = os.path.expanduser("~/.triton/cache")
                if os.path.exists(triton_cache_dir):
                    # Only clear if cache is getting too large (> 2GB)
                    cache_size = sum(
                        os.path.getsize(os.path.join(dirpath, filename))
                        for dirpath, dirnames, filenames in os.walk(triton_cache_dir)
                        for filename in filenames
                    ) / (1024**3)
                    if cache_size > 2.0:  # 2GB threshold
                        shutil.rmtree(triton_cache_dir, ignore_errors=True)
                        logger.debug(f"Cleared large Triton cache ({cache_size:.2f}GB)")
            except Exception:
                pass  # Ignore errors in cache clearing
        except AttributeError:
            # Triton might not have these methods
            pass

        module_name = f"test_kernel_{op_name}_{attempt}"
        try:
            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")

            # Always save the new kernel code for each attempt
            is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code
            if is_triton:
                full_code = self._prepare_triton_code(kernel_code)
            else:
                full_code = self._prepare_torch_code(kernel_code)

            with open(kernel_file, "w") as f:
                f.write(full_code)
            logger.debug(f"Saved kernel to: {kernel_file}")

            # Clear CUDA memory before compilation
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            spec = importlib.util.spec_from_file_location(module_name, kernel_file)
            if spec is None:
                raise RuntimeError(f"Failed to create module spec for {kernel_file}")

            module = importlib.util.module_from_spec(spec)
            if spec.loader is None:
                raise RuntimeError(f"No loader available for module spec {kernel_file}")

            # Add to sys.modules so triton can find it
            sys.modules[module_name] = module

            try:
                # Clear memory right before module execution
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                spec.loader.exec_module(module)

                expected_name = f"{op_name}_kernel_impl"
                if hasattr(module, expected_name):
                    compiled_kernel = getattr(module, expected_name)
                else:
                    available_functions = [
                        name
                        for name in dir(module)
                        if callable(getattr(module, name)) and not name.startswith("_")
                    ]
                    raise ValueError(
                        f"Expected function '{expected_name}' not found. Available: {available_functions}"
                    )

                # Force CUDA cache clear after compilation
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                # Clear Triton cache after compilation
                try:
                    if hasattr(triton, 'cache'):
                        triton.cache.clear()
                except AttributeError:
                    pass

            finally:
                # Always clean up module from sys.modules
                if module_name in sys.modules:
                    del sys.modules[module_name]

            # Force garbage collection to clean up any remaining references
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            correct_count = 0
            total_count = 0

            for test_idx, test in enumerate(test_cases):
                # Initialize args and kwargs with defaults for error reporting
                args = []
                kwargs = {}

                try:
                    # Make copies to avoid mutating original test data
                    args = test.args
                    kwargs = test.kwargs

                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"    Running test {test_idx + 1}/{len(test_cases)}")

                    # Clear cache before reference computation
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    ref_result = op(*args, **kwargs)

                    # Clear CUDA cache after reference and before kernel execution
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()

                    kernel_result = compiled_kernel(*args, **kwargs)

                    # Clear cache after kernel execution
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    torch.testing.assert_close(ref_result, kernel_result, equal_nan=True)
                    correct_count += 1
                    logger.debug(f"    âœ“ Test {test_idx + 1} passed: {ref_result.shape} {ref_result.dtype}")

                except Exception as e:
                    logger.debug(f"    âœ— Test {test_idx + 1} failed: {str(e)}")

                    # Clear cache even on failure to prevent memory accumulation
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    # Safely get args/kwargs for error reporting
                    try:
                        test_input_info = f"args={[arg.shape if hasattr(arg, 'shape') else arg for arg in args]}, kwargs={kwargs}"
                    except:
                        test_input_info = f"Test case {test_idx + 1} (args/kwargs not available)"

                    feedback_info["test_errors"].append(
                        {
                            "test_input": test_input_info,
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "traceback": traceback.format_exc(),
                        }
                    )

                total_count += 1

            is_correct = correct_count == total_count and total_count > 0
            feedback_info["summary"] = f"{correct_count}/{total_count} tests passed"

            return is_correct, feedback_info

        except Exception as e:
            logger.error("    âœ— Compilation failed:")
            logger.error(f"      Error: {str(e)}")

            # Clear cache on compilation failure too
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            feedback_info["compilation_error"] = str(e)
            feedback_info["summary"] = "Compilation failed"
            return False, feedback_info

        finally:
            # Final cleanup - ensure module is removed and cache is cleared
            if module_name in sys.modules:
                del sys.modules[module_name]

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

    def get_conversation_manager(self, op_name: str) -> Optional[ConversationManager]:
        """Get conversation manager for debugging."""
        return self.conversation_managers.get(op_name)

    def __getitem__(self, key):
        if key in self.compiled_kernels:
            return self.compiled_kernels[key]
        raise KeyError(f"No kernel implementation found for {key}")

    def __contains__(self, key):
        return key in self.compiled_kernels
