# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD 3-Clause license found in the
# LICENSE file in the root directory of this source tree.

import datetime
import gc
import importlib.util
import logging
import os
import sys
import torch
import traceback
from typing import Callable, Dict, List, Optional
import triton

from .base import Backend
from ..conversation_manager import ConversationManager

logger = logging.getLogger(__name__)


class LLMRelayBackend(Backend):
    """
    Backend that uses LLMKernelGenerator to communicate with local plugboard server.
    This backend will eventually replace the LLMBackend that uses direct Anthropic API calls.
    """

    def __init__(self, model: str = "gcp-claude-4-sonnet", debug_mode: bool = False) -> None:
        super().__init__("llm-relay")
        self.compiled_kernels: Dict[str, Callable] = {}
        self.model = model
        self.debug_mode = debug_mode
        self.conversation_managers: Dict[str, ConversationManager] = {}

        # Set CUDA environment variables for better error debugging
        os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
        os.environ["TORCH_USE_CUDA_DSA"] = "1"

        # Create generated_kernels directory with conversation support
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.kernels_dir = f"generated_kernels/llm_relay_run_{timestamp}"
        os.makedirs(self.kernels_dir, exist_ok=True)

        # Create conversations subdirectory for debug logs
        if self.debug_mode:
            self.conversations_dir = os.path.join(self.kernels_dir, "conversations")
            os.makedirs(self.conversations_dir, exist_ok=True)

        # Create README for this run
        readme_path = os.path.join(self.kernels_dir, "README.md")
        with open(readme_path, "w") as f:
            f.write(
                f"""# Generated Kernels - LLM Relay - {timestamp}

This directory contains PyTorch/Triton kernels generated by the LLM Relay Backend.

## Run Info
- Timestamp: {timestamp}
- Backend: LLM Relay
- Model: {model}
- Server: Local plugboard server (localhost:11434)
- Debug Mode: {debug_mode}
- Conversation History: {'Enabled' if debug_mode else 'Disabled'}

## Files
Each `<op_name>_kernel.py` file contains the complete generated kernel code for that operation, including:
- All necessary imports
- Triton kernel implementation (if applicable)
- Wrapper function that matches PyTorch operation signature

## Conversation Logs
{'Conversation logs saved to conversations/ subdirectory when debug mode is enabled.' if debug_mode else 'Enable debug mode to generate conversation logs.'}

## Server Setup
This backend requires the plugboard server to be running:
```
buck run @//mode/inplace run_plugboard_server -- --model gcp-claude-4-sonnet --pipeline usecase-dev-ai-user
```

## Usage
You can inspect these files to debug kernel generation, manually test implementations, or understand what the LLM produced.
"""
            )

        logger.info(f"Saving LLM Relay generated kernels to: {self.kernels_dir}")
        if debug_mode:
            logger.info(f"Debug mode enabled - conversation logs will be saved to: {self.conversations_dir}")

    def compile_kernel_from_string(
        self, kernel_code: str, op_name: str, attempt: int = 1
    ) -> Callable:
        """Compile a kernel from string code and return a callable."""
        try:
            is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code

            if is_triton:
                full_code = self._prepare_triton_code(kernel_code)
            else:
                full_code = self._prepare_torch_code(kernel_code)

            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")
            with open(kernel_file, "w") as f:
                f.write(full_code)

            logger.debug(f"Saved kernel to: {kernel_file}")

            spec = importlib.util.spec_from_file_location(f"kernel_{op_name}", kernel_file)
            if spec is None:
                raise RuntimeError(f"Failed to create module spec for {kernel_file}")

            module = importlib.util.module_from_spec(spec)
            if spec.loader is None:
                raise RuntimeError(f"No loader available for module spec {kernel_file}")

            spec.loader.exec_module(module)

            kernel_func = self._find_kernel_function(module, op_name)

            return kernel_func

        except Exception as e:
            raise RuntimeError(f"Failed to compile kernel for {op_name}: {str(e)}")

    def _prepare_triton_code(self, kernel_code: str) -> str:
        """Prepare Triton kernel code with necessary imports."""
        imports = """
import torch
import triton
import triton.language as tl
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _prepare_torch_code(self, kernel_code: str) -> str:
        """Prepare regular PyTorch kernel code with necessary imports."""
        imports = """
import torch
import torch.nn.functional as F
"""
        if "import torch" not in kernel_code:
            kernel_code = imports + kernel_code
        return kernel_code

    def _find_kernel_function(self, module, op_name: str) -> Callable:
        """Find the main kernel function in the compiled module."""
        expected_name = f"{op_name}_kernel_impl"

        if hasattr(module, expected_name):
            return getattr(module, expected_name)

        available_functions = [
            name
            for name in dir(module)
            if callable(getattr(module, name)) and not name.startswith("_")
        ]

        raise ValueError(
            f"Expected function '{expected_name}' not found in kernel code for {op_name}. "
            f"Available functions: {available_functions}. "
            f"Please ensure the LLM generated code follows the naming convention: {op_name}_kernel_impl"
        )

    def add_kernel(self, op, kernel_code: str, op_name: str):
        """Add a kernel implementation for a specific operator."""
        # Clear CUDA cache before final compilation to ensure clean state
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        compiled_kernel = self.compile_kernel_from_string(kernel_code, op_name, attempt=1)
        self.compiled_kernels[op] = compiled_kernel

        # Final cleanup after successful compilation
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _nuclear_memory_reset(self, op_name: str, attempt: int):
        """Nuclear option: Completely reset GPU memory state."""
        if not torch.cuda.is_available():
            return

        try:
            # Log memory state before reset
            allocated_before = torch.cuda.memory_allocated() / (1024**3)
            reserved_before = torch.cuda.memory_reserved() / (1024**3)

            # Step 1: Clear all PyTorch caches
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            # Step 2: Reset memory allocator completely
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()

            # Step 3: Nuclear option - force garbage collection
            import gc
            gc.collect()

            # Step 4: Clear all CUDA streams and events
            try:
                torch.cuda.empty_cache()
                # Force synchronization on all devices
                for device_id in range(torch.cuda.device_count()):
                    with torch.cuda.device(device_id):
                        torch.cuda.synchronize()
                        torch.cuda.empty_cache()
            except Exception:
                pass

            # Step 5: Clear Triton cache completely
            try:
                if hasattr(triton, 'cache'):
                    triton.cache.clear()
                if hasattr(triton, 'compiler') and hasattr(triton.compiler, 'clear_cache'):
                    triton.compiler.clear_cache()

                # Nuclear: Remove entire triton cache directory
                import shutil
                triton_cache_dir = os.path.expanduser("~/.triton/cache")
                if os.path.exists(triton_cache_dir):
                    try:
                        shutil.rmtree(triton_cache_dir, ignore_errors=True)
                        logger.debug(f"[{op_name}:{attempt}] Completely cleared Triton cache directory")
                    except Exception:
                        pass
            except Exception:
                pass

            # Log final memory state
            allocated_after = torch.cuda.memory_allocated() / (1024**3)
            reserved_after = torch.cuda.memory_reserved() / (1024**3)

            logger.debug(f"[{op_name}:{attempt}] Memory reset: "
                        f"Allocated {allocated_before:.2f}GB -> {allocated_after:.2f}GB, "
                        f"Reserved {reserved_before:.2f}GB -> {reserved_after:.2f}GB")

        except Exception as e:
            logger.warning(f"[{op_name}:{attempt}] Memory reset failed: {e}")

    def test_kernel_correctness(
        self, op, kernel_code: str, test_cases: List, attempt: int = 1
    ) -> tuple[bool, Dict]:
        """Test kernel correctness and return detailed feedback."""
        op_str = str(op)
        if "aten." in op_str:
            op_name = op_str.split("aten.")[-1].split(".")[0]
        else:
            op_name = op_str.split(".")[-1]

        # CRITICAL DEBUG: Check test_cases state
        try:
            test_cases_count = len(test_cases)
            print(f"üîç test_kernel_correctness attempt {attempt}: received {test_cases_count} test cases")
        except:
            print(f"üîç test_kernel_correctness attempt {attempt}: received test_cases (unknown count)")

        if not test_cases:
            print(f"‚ùå CRITICAL BUG: test_cases is empty on attempt {attempt} for {op_name}")

        feedback_info = {
            "compilation_error": None,
            "test_errors": [],
            "summary": None,
        }

        # Nuclear memory reset for each attempt
        self._nuclear_memory_reset(op_name, attempt)

        # Check available memory and abort if too low
        if torch.cuda.is_available():
            try:
                # Get available memory (this might not be 100% accurate but gives an idea)
                allocated = torch.cuda.memory_allocated()
                reserved = torch.cuda.memory_reserved()
                total_memory = torch.cuda.get_device_properties(0).total_memory
                available_approx = total_memory - reserved

                logger.debug(f"[{op_name}:{attempt}] Memory status: {available_approx/1024**3:.2f}GB available of {total_memory/1024**3:.2f}GB total")

                # If less than 2GB available, this is very risky for complex operations
                if available_approx < 2 * 1024**3:  # 2GB
                    logger.warning(f"[{op_name}:{attempt}] Very low GPU memory: {available_approx/1024**3:.2f}GB available")
                    # Try one more aggressive reset
                    self._nuclear_memory_reset(op_name, attempt)

                    # If still low after reset, this operation might be too complex
                    allocated_after = torch.cuda.memory_allocated()
                    reserved_after = torch.cuda.memory_reserved()
                    available_after = total_memory - reserved_after

                    if available_after < 1.5 * 1024**3:  # 1.5GB
                        logger.error(f"[{op_name}:{attempt}] Critical memory shortage: {available_after/1024**3:.2f}GB available after reset")

                        # For certain complex operations, skip early to avoid crashes
                        complex_ops = ['col2im', 'conv2d', 'conv_transpose2d', 'batch_norm', 'layer_norm', 'scaled_dot_product_attention']
                        if op_name in complex_ops:
                            logger.error(f"[{op_name}:{attempt}] Skipping complex operation {op_name} due to memory constraints")
                            feedback_info["compilation_error"] = f"Skipped complex operation {op_name}: Only {available_after/1024**3:.2f}GB GPU memory available, need at least 2GB for complex ops"
                            feedback_info["summary"] = f"Skipped complex operation due to insufficient memory"
                            return False, feedback_info

                        feedback_info["compilation_error"] = f"Insufficient GPU memory: {available_after/1024**3:.2f}GB available, need at least 1.5GB"
                        feedback_info["summary"] = "Insufficient GPU memory"
                        return False, feedback_info

            except Exception as e:
                logger.warning(f"[{op_name}:{attempt}] Memory check failed: {e}")

        module_name = f"test_kernel_{op_name}_{attempt}"
        try:
            kernel_file = os.path.join(self.kernels_dir, f"{op_name}_kernel_attempt_{attempt}.py")

            # Always save the new kernel code for each attempt
            is_triton = "triton.jit" in kernel_code or "@triton.jit" in kernel_code
            if is_triton:
                full_code = self._prepare_triton_code(kernel_code)
            else:
                full_code = self._prepare_torch_code(kernel_code)

            with open(kernel_file, "w") as f:
                f.write(full_code)
            logger.debug(f"Saved kernel to: {kernel_file}")

            # Clear CUDA memory before compilation
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            spec = importlib.util.spec_from_file_location(module_name, kernel_file)
            if spec is None:
                raise RuntimeError(f"Failed to create module spec for {kernel_file}")

            module = importlib.util.module_from_spec(spec)
            if spec.loader is None:
                raise RuntimeError(f"No loader available for module spec {kernel_file}")

            # Add to sys.modules so triton can find it
            sys.modules[module_name] = module

            try:
                # Clear memory right before module execution
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                spec.loader.exec_module(module)

                expected_name = f"{op_name}_kernel_impl"
                if hasattr(module, expected_name):
                    compiled_kernel = getattr(module, expected_name)
                else:
                    available_functions = [
                        name
                        for name in dir(module)
                        if callable(getattr(module, name)) and not name.startswith("_")
                    ]
                    raise ValueError(
                        f"Expected function '{expected_name}' not found. Available: {available_functions}"
                    )

                # Force CUDA cache clear after compilation
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                # Clear Triton cache after compilation
                try:
                    if hasattr(triton, 'cache'):
                        triton.cache.clear()
                except AttributeError:
                    pass

            finally:
                # Always clean up module from sys.modules
                if module_name in sys.modules:
                    del sys.modules[module_name]

            # Force garbage collection to clean up any remaining references
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            correct_count = 0
            total_count = 0

            for test_idx, test in enumerate(test_cases):
                # Initialize args and kwargs with defaults for error reporting
                args = []
                kwargs = {}

                try:
                    # Make copies to avoid mutating original test data
                    args = test.args
                    kwargs = test.kwargs

                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"    Running test {test_idx + 1}/{len(test_cases)}")

                    # Clear cache before reference computation
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    ref_result = op(*args, **kwargs)

                    # Clear CUDA cache after reference and before kernel execution
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()

                    kernel_result = compiled_kernel(*args, **kwargs)

                    # Clear cache after kernel execution
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    torch.testing.assert_close(ref_result, kernel_result, equal_nan=True)
                    correct_count += 1
                    logger.debug(f"    ‚úì Test {test_idx + 1} passed: {ref_result.shape} {ref_result.dtype}")

                except Exception as e:
                    logger.debug(f"    ‚úó Test {test_idx + 1} failed: {str(e)}")

                    # Clear cache even on failure to prevent memory accumulation
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    # Safely get args/kwargs for error reporting
                    try:
                        test_input_info = f"args={[arg.shape if hasattr(arg, 'shape') else arg for arg in args]}, kwargs={kwargs}"
                    except:
                        test_input_info = f"Test case {test_idx + 1} (args/kwargs not available)"

                    feedback_info["test_errors"].append(
                        {
                            "test_input": test_input_info,
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "traceback": traceback.format_exc(),
                        }
                    )

                total_count += 1

            is_correct = correct_count == total_count and total_count > 0
            feedback_info["summary"] = f"{correct_count}/{total_count} tests passed"

            return is_correct, feedback_info

        except Exception as e:
            logger.error("    ‚úó Compilation failed:")
            logger.error(f"      Error: {str(e)}")

            # Clear cache on compilation failure too
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            feedback_info["compilation_error"] = str(e)
            feedback_info["summary"] = "Compilation failed"
            return False, feedback_info

        finally:
            # Final cleanup - ensure module is removed and cache is cleared
            if module_name in sys.modules:
                del sys.modules[module_name]

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

    def get_conversation_manager(self, op_name: str) -> Optional[ConversationManager]:
        """Get conversation manager for debugging."""
        return self.conversation_managers.get(op_name)

    def __getitem__(self, key):
        if key in self.compiled_kernels:
            return self.compiled_kernels[key]
        raise KeyError(f"No kernel implementation found for {key}")

    def __contains__(self, key):
        return key in self.compiled_kernels
